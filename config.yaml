# ==========================================
# 7B Model "Sprint" Configuration
# Hardware: 2x NVIDIA RTX Blackwell (96GB)
# Goal: Train in ~30 Days
# ==========================================

# ==========================================
# Model Architecture (Llama-2/Mistral Standard)
# ==========================================
vocab_size: 128256              # Llama-3 style tokenizer for data efficiency
hidden_size: 4096               # Standard 7B width
intermediate_size: 11008        # SwiGLU ratio (~2.7x)
num_hidden_layers: 32           # Deeper for reasoning
num_attention_heads: 32         # Query heads
num_key_value_heads: 8          # GQA (Grouped Query Attention) for fast inference
max_seq_len: 8192               # Long context
rms_norm_eps: 1.0e-5
rope_theta: 500000.0            # Extended context RoPE
tie_word_embeddings: false

# ==========================================
# Training Parameters
# ==========================================
# Learning Rate Strategy (Stability)
learning_rate: 1.5e-4           # Lower LR for larger model
min_lr: 1.5e-5                  # 10% of max LR
weight_decay: 0.1
max_grad_norm: 1.0
warmup_steps: 2000              # Explicit warmup steps

# Hardware Optimization (Speed First)
# Strategy: Turn OFF checkpointing to gain ~30% speed
# Sacrifice physical batch size to fit in VRAM
activation_checkpointing: false # CRITICAL: False = Speed
batch_size: 2                   # Per GPU batch size (low to prevent OOM)

# Global Batch Size Math
# Goal: ~2.1 Million tokens per step (Standard for 7B)
# Formula: 2.1M / (2 GPUs * 2 Batch * 8192 Seq) = 64
gradient_accumulation_steps: 64

# ==========================================
# Data Parameters
# ==========================================
# Train on a random sample of ~60B tokens from FineWeb
data_path: "hf://datasets/HuggingFaceFW/fineweb/sample/100BT"
total_tokens: 60000000000       # 60 billion tokens
num_workers: 0                  # Must be 0 for IterableDataset

# ==========================================
# Tokenizer
# ==========================================
tokenizer_name: "unsloth/Llama-3.2-1B"  # Llama-3 tokenizer (ungated, no login required)

# ==========================================
# Mixed Precision
# ==========================================
dtype: "bf16"                   # bf16 preferred for training stability

# ==========================================
# Checkpointing
# ==========================================
checkpoint_dir: "./checkpoints"
checkpoint_interval: 500        # More frequent for long training
log_interval: 10

# Resume training (set via command line with --resume)
resume: false
